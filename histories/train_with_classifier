import os

import numpy as np
import torch.nn as nn
import torch
import torch.cuda
import torchvision
from sklearn import svm
from sklearn.linear_model import SGDClassifier
from torch.utils.data import DataLoader, random_split
from torch import optim
from sklearn.metrics import precision_score, recall_score
from tqdm import tqdm
from src.model import AENet
from src.celeba_dataset import CelebaDataset


def spoof_condition(values, threshold):
    # return (torch.argmax(values, dim=1) == 1) & (values[:, 1] >= threshold)
    return values[:, 1] >= threshold


def train_model(config, data_dir, epochs, last_epoch=None):
    train_files = os.listdir(data_dir)
    device = torch.device("cuda:0")
    model = AENet()
    model = nn.DataParallel(model)
    model.to(device)

    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([0.2, 5]).to(device))
    optimizer = optim.Adam(model.parameters(), lr=config["lr"], weight_decay=config["weight_decay"])

    checkpoint_dir = "C:/Users/vlank/PycharmProjects/Antispoof1/checkpoints_clf"
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_file = os.path.join(checkpoint_dir, f"checkpoint_{last_epoch}_clf.pt")

    if (last_epoch is None) or not os.path.exists(checkpoint_file):
        print("Last epoch file does not exist!")
        start_epoch = 0
    else:
        checkpoint_data = torch.load(checkpoint_file)
        start_epoch = checkpoint_data["epoch"] + 1
        model.load_state_dict(checkpoint_data["model_state_dict"])
        optimizer.load_state_dict(checkpoint_data["optimizer_state_dict"])

    clf = SGDClassifier(loss='log_loss', class_weight={0: 1, 1: 10})
    for epoch in range(start_epoch, start_epoch + epochs):
        print(f"EPOCH {epoch}")
        model.train()
        for i, file_name in enumerate(train_files):
            print(f"package {i}...")
            dataset = CelebaDataset(os.path.join(train_dir, file_name))
            dataloader = DataLoader(dataset, batch_size=config["batch_size"], shuffle=True)
            for images, labels in tqdm(dataloader):
                images, labels = images.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

        model.eval()
        for i, file_name in enumerate(train_files):
            print(f"classifier {i}...")
            dataset = CelebaDataset(os.path.join(train_dir, file_name))
            dataloader = DataLoader(dataset, batch_size=config["batch_size"], shuffle=False)
            for images, labels in tqdm(dataloader):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                clf.partial_fit(outputs.detach().cpu().numpy(), torch.argmax(labels, dim=1).detach().cpu().numpy(),
                                np.unique(labels.detach().cpu()))

        checkpoint_data = {
            "epoch": epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            'classifier': clf
        }

        checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_{epoch}_clf.pt")
        torch.save(checkpoint_data, checkpoint_path)


if __name__ == '__main__':
    print(torchvision.__version__)
    print(torch.__version__)
    print(torch.version.cuda)
    train_dir = "D:/AK/CelebA_Spoof/transformed_dataset/train"
    config = {
        "lr": 1e-6,
        "batch_size": 16,
        "weight_decay": 1e-1
    }
    train_model(config=config, data_dir=train_dir, epochs=10, last_epoch=1)
